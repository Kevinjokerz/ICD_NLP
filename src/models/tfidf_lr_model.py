"""
tfidf_lr_model.py - TF-IDF + OVR Logistic Regression

Purpose
-------
Wrap TF-IDF vectorizers (word/char) and OVR Logistic Regression behind ProbModel
for unified training/evaluation/saving

Conventions
-----------
- Reproducibility: fixed seed; version logged; pickle protocol pinned.
- Portability: relative paths only; PHI-safe (no raw TEXT or vocab in logs).
- Metrics-first: Micro/Macro-F1, Precision@k via shared evaluator; fixed threshold
  for fair comparison across models.

Inputs
------
- data/splits/{train, val, test}.jsonl
- data/processed/label_map.json

Outputs
-------
- models/{run}/tfidf_word.pkl, tfidf_char.pkl, logreg_ovr.pkl, pickle_meta.json
- reports/baseline_metrics.csv (one row per evaluation call).

Author
------
Kevin Vo
"""

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Optional, Tuple, List, Union

import logging
import pickle
import json
import numpy as np
from scipy import sparse as sp
import sys
import sklearn
from sklearn.feature_extraction.text import TfidfVectorizer
# =============================================================================
# Project utilities
# =============================================================================
from src.utils import setup_logger, ensure_parent_dir, to_relpath
from src.utils import set_seed as set_global_seed
from src.common import (
    VectorizeConfig, make_vectorizers, fit_transform_splits, save_vectorizers,
    prepare_split, evaluate_and_save, transform_texts
)
from src.protocol import ProbModel
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier

# =============================================================================
# Config dataclasses
# =============================================================================

@dataclass(frozen=True)
class TFIDFArgs:
    """TF-IDF configuration"""
    vec_mode: str = "both"              # {"word", "char", "both", "none"}
    min_df: Union[float, int] = 2
    word_ngram: Tuple[int, int] = (1, 2)
    char_ngram: Tuple[int, int] = (3, 5)
    word_max: int = 50_000
    char_max: int = 30_000
    dtype: "np.dtype" = np.float32        # features must be float32

@dataclass(frozen=True)
class LRArgs:
    """Logistic Regression (base classifier) configuration."""
    C: float = 2.0
    solver: str = "liblinear"           # {"liblinear", "saga", "lbfgs", "newton-cg", "sag"}
    max_iter: int = 400
    random_state: Optional[int] = None
    n_jobs: Optional[int] = None        # set when using "saga"
    class_weight: Optional[str] = None  # e.g., "balanced" or None

@dataclass(frozen=True)
class EvalArgs:
    """Evaluation configuration (fixed threshold for fairness)."""
    thr: float = 0.5                    # fixed threshold policy
    threshold_policy: str = "fixed"     # {"fixed", "global_val", "per_label_val"}
    ks: Tuple[int, int] = (8, 15)       # Precision@k (reporting handled in evaluator)
    min_pos: int = 10                   # per-label tuning guard

@dataclass(frozen=True)
class CommonArgs:
    """Common run-time configuration."""
    run: str = "tfidf_lr_v1"
    train: str = "data/splits/train.jsonl"
    val: Optional[str] = "data/splits/val.jsonl"
    test: Optional[str] = "data/splits/test.jsonl"
    label_map: str = "data/processed/label_map.json"
    protocol: int = 4
    seed: int = 42
    log_level: str = "INFO"             # {"DEBUG", "INFO", "WARNING", "ERROR"}

# =============================================================================
# I/O helpers
# =============================================================================

def _load_split(
        path: Union[str, Path],
        *,
        label_map: Union[str, Path],
        logger: Optional[logging.Logger] = None,
        return_meta: bool = False,
        return_label_map: bool = False,
) -> Tuple[List[str], np.ndarray] | Tuple[List[str], np.ndarray, dict] | Tuple[List[str], np.ndarray, dict, dict]:
    """
    Load a split and return (texts, multi-hot Y).

    Parameters
    ----------
    path    : Union[str, Path]
        Split file path (.jsonl)
    label_map: Union[str, Path]
        Path to label_map.json generated by your pipeline (required by prepare_split).
    logger  : logging.Logger, optional
        Project logger (No raw TEXT logged).

    Returns
    -------
    texts   : list[str]
        De-identified note texts.
    Y       : np.ndarray
        Multi-hot labels, shape (N, K) dtype in {uint8, int8, int32, bool}
    """
    if logger is None:
        logger = logging.getLogger(__name__)
    
    p = Path(path)
    lm_path = Path(label_map)

    assert p.exists(), f"[IO] Split not found: {to_relpath(p)}"
    assert lm_path.exists(), f"[IO] label_map not found: {to_relpath(lm_path)}"

    texts, Y, meta, lm_map = prepare_split(p, label_map_path=lm_path, logger=logger)
    assert isinstance(Y, np.ndarray) and Y.ndim == 2, "[CHECK] Y must be 2D (N, K)"

    if Y.dtype not in (np.uint8, np.bool_, bool):
        Y = Y.astype(np.uint8, copy=False)
    
    out = [texts, Y]
    if return_meta:
        out.append(meta)
    if return_label_map:
        out.append(lm_map)
    
    logger.info(f"[SPLIT] {to_relpath(p)} -> N={len(texts)}, K={Y.shape[1]}")
    return tuple(out)

# =============================================================================
# Feature building
# =============================================================================

def build_features(
        *,
        tfidf: TFIDFArgs,
        texts_tr: List[str],
        texts_va: Optional[List[str]],
        texts_te: Optional[List[str]],
        logger: Optional[logging.Logger] = None,
) -> Tuple[Dict[str, sp.csr_matrix], Any, Any]:
    """
    Fit TF-IDF on train; transform val/test; return matrices + vectorizers.

    Returns
    -------
    mats    : dict[str, sp.csr_matrix]
        Keys in {"train", "val", "test"}; CSR float32 matrices.
    vec_word, vec_char : TfidfVectorizer | None
        Word- and char-level TF-IDF vectorizers (None depending on vec_mode).
    """
    if logger is None:
        logger = logging.getLogger(__name__)

    cfg = VectorizeConfig(
        vec_mode=tfidf.vec_mode,
        word_ngram=tfidf.word_ngram,
        char_ngram=tfidf.char_ngram,
        word_max_features=tfidf.word_max,
        char_max_features=tfidf.char_max,
        min_df=tfidf.min_df,
        lowercase=True,
        strip_accents="unicode",
        sublinear_tf=True,
        dtype=tfidf.dtype
    )

    vec_word, vec_char = make_vectorizers(cfg=cfg)
    mats = fit_transform_splits(
        vec_word=vec_word, 
        vec_char=vec_char, 
        texts_tr=texts_tr,
        texts_va=texts_va,
        texts_te=texts_te,
        to_dtype=tfidf.dtype,
        logger=logger
    )

    assert sp.isspmatrix_csr(mats["train"]), "[CHECK] train features must be CSR."

    if "val" in mats:
        assert mats["val"].shape[1] == mats["train"].shape[1], "[CHECK] val D != train D"
    if "test" in mats:
        assert mats["test"].shape[1] == mats["train"].shape[1], "[CHECK] test D != train D"
    
    assert mats["train"].dtype == np.float32, "[CHECK] TF-IDF dtype must be float32"
    logger.info(
        "[TFIDF] " + ", ".join(f"{k}={v.shape[0]}x{v.shape[1]}" for k, v in mats.items())
    )
    return mats, vec_word, vec_char

# =============================================================================
# Modeling
# =============================================================================
class TfidfOvRModel:
    def __init__(self, tfidf: TFIDFArgs, lr: LRArgs) -> None:
        """
        Adapter: TF-IDF (word/char) + One-vs-Rest Logistic Regression.
        """
        self.tfidf = tfidf
        self.lr = lr

        # Vectorizers
        self.vw: Optional[TfidfVectorizer] = None
        self.vc: Optional[TfidfVectorizer]= None

        # Classifier
        self.clf: Optional[OneVsRestClassifier] = None

        # Label Dimension (K)
        self.K: Optional[int] = None

        # --- persistable pickle protocol (default = 4) ---
        self.pickle_protocol: int = 4
    
    def set_pickle_protocol(self, protocol: int) -> None:
        """
        Set pickle protocol to use when saving artifacts.

        Parameters
        ----------
        protocol : int
            e.g., 4 or higher (Py3+)
        """
        assert isinstance(protocol, int) and protocol >= 4, f"[CHECK] protcol must be an integer and >= 4"
        self.pickle_protocol = int(protocol)
        
    
    def fit(self, texts: List[str], Y: np.ndarray,
            logger: Optional[logging.Logger] = None
    ) -> TfidfOvRModel:
        """
        Train TF-IDF vectorizers (word/char) and OvR Logistic Regression classifier.

        Parameters
        ----------
        texts   : List[str]
            De-identified discharge summaries for training.
        Y       : np.ndarray
            Multi-hto label matrix with shape (N, K) dtype in {uint8, bool, int8, int32}.
            Values must be in {0, 1}.
        logger  : logging.Logger, optional
            Project logger. If None, a module logger is used
        
        Returns
        -------
        TfidfOvRModel
            the fitted model, with trained vectorizers and classifier.
            The label dimension `K` is set from `Y.shape[1]`.
        """
        if logger is None:
            logger = logging.getLogger(__name__)
        assert isinstance(Y, np.ndarray) and Y.ndim == 2, "[CHECK] Y must be 2D (N, K)"

        self.K = int(Y.shape[1])
        cfg = VectorizeConfig(
            vec_mode=self.tfidf.vec_mode,
            word_max_features=self.tfidf.word_max,
            char_max_features=self.tfidf.char_max,
            word_ngram=self.tfidf.word_ngram,
            char_ngram=self.tfidf.char_ngram,
            min_df=self.tfidf.min_df,
            lowercase=True,
            strip_accents="unicode",
            sublinear_tf=True,
            dtype=self.tfidf.dtype
        )

        self.vw, self.vc = make_vectorizers(cfg=cfg)
        mats = fit_transform_splits(
            vec_word=self.vw,
            vec_char=self.vc,
            texts_tr=texts,
            texts_va=None,
            texts_te=None,
            to_dtype=self.tfidf.dtype,
            logger=logger
        )

        X = mats["train"]
        assert sp.isspmatrix_csr(X), "[CHECK] Train features must be CSR."
        assert X.dtype == np.float32, "[CHECK] TF-IDF dtype must be float32."
        X.sort_indices()

        kwargs = dict(C=self.lr.C, solver=self.lr.solver, max_iter=self.lr.max_iter)
        if self.lr.random_state is not None:
            kwargs["random_state"] = self.lr.random_state
        if self.lr.n_jobs is not None and self.lr.solver in {"liblinear", "sag", "saga"}:
            kwargs["n_jobs"] = self.lr.n_jobs
        if self.lr.class_weight is not None:
            kwargs["class_weight"] = self.lr.class_weight
        
        base = LogisticRegression(**kwargs)
        self.clf = OneVsRestClassifier(base)
        self.clf.fit(X, Y)

        assert hasattr(self.clf, "classes_")
        logger.info(f"[FIT] X={X.shape}, K={self.K}, solver={self.lr.solver}, C={self.lr.C}")

        return self

    def predict_proba(self, texts: List[str], 
                      logger: Optional[logging.Logger]=None
    ) -> np.ndarray:
        """
        Predict per-label probabilities for input texts.

        Parameters
        ----------
        texts   : List[str]
            De-identified notes to score. Length M.
        
        Returns
        -------
        np.ndarray
            Probability matrix `P` with shape (M, K), dtype float32, values in [0, 1].
        
        Notes
        -----
        - Uses the trained TF-IDF vectorizers to transform `texts`.
        - `K` matches the training label dimension.
        - No thresholding applied here; downstream evaluator may binarize.
        """
        if logger is None:
            logger = logging.getLogger(__name__)
        if not texts:
            k = int(self.K or 0)
            return np.empty((0, k), dtype=np.float32)

        assert self.clf is not None and (self.vw is not None or self.vc is not None), "[CHECK] Model/vectorizer not fitted"

        X = transform_texts(
            vec_word=self.vw, vec_char=self.vc, texts=texts, 
            to_dtype=self.tfidf.dtype, logger=logger
        )
        assert sp.isspmatrix_csr(X), "[CHECK] Test features must be CSR"
        assert X.dtype == np.float32, "[CHECK] TF-IDF dtype must be float32"
        X.sort_indices()

        P = self.clf.predict_proba(X).astype(np.float32, copy=False)
        assert P.ndim == 2 and P.shape[0] == len(texts), "[CHECK] bad proba shape."
        if self.K is not None:
            assert P.shape[1] == self.K, "[CHECK] K mismatch"
        
        logger.debug(f"predict_proba: M={len(texts)}, K={P.shape[1]}")
        return P
    
    def save(self, path: Path, *, logger: Optional[logging.Logger]=None) -> None:
        """
        Persist model artifact under `path`:
        - TF-IDF vectorizers (word/char) via `save_vectorizers(...)`
        - OVR classifier pickle
        - `pickle_meta.json` with config, label dim, and version
        """

        if logger is None:
            logger = logging.getLogger(__name__)

        assert (self.vw is not None or self.vc is not None) and self.clf is not None, "[CHECK] Model not fitted - nothing to save."

        ensure_parent_dir(path /"placeholder")
        out_dir = Path(path)
        out_dir.mkdir(parents=True, exist_ok=True)

        save_vectorizers(
            vec_word=self.vw, vec_char=self.vc, dir_path=out_dir.parent, 
            run_name=out_dir.name, protocol=self.pickle_protocol, logger=logger
        )

        with open(out_dir / "logreg_ovr.pkl", "wb") as f:
            pickle.dump(self.clf, f, protocol=self.pickle_protocol)
        
        tfidf_cfg = dict(self.tfidf.__dict__)
        tfidf_cfg["dtype"] = np.dtype(tfidf_cfg.get("dtype", np.float32)).name
        tfidf_cfg["word_ngram"] = list(tfidf_cfg.get("word_ngram", (1, 2)))
        tfidf_cfg["char_ngram"] = list(tfidf_cfg.get("char_ngram", (3, 5)))

        lr_cfg = dict(self.lr.__dict__)
        for k, v in lr_cfg.items():
            if isinstance(v, (np.integer, np.floating, np.bool_)):
                lr_cfg[k] = v.item()
            
        meta = {
            "K": self.K,
            "tfidf": tfidf_cfg,
            "lr": lr_cfg,
            "protocol": int(self.pickle_protocol),
            "versions": {
                "python": sys.version.split()[0],
                "numpy": np.__version__,
                "sklearn": sklearn.__version__
            },
            "feature_dim": len(getattr((self.vw or self.vc), "vocabulary_", {})) if (self.vw or self.vc) else None,
            "vocab_sizes": {
                "word": len(getattr(self.vw, "vocabulary_", {})) if self.vw else 0,
                "char": len(getattr(self.vc, "vocabulary_", {})) if self.vc else 0,
            },
            "paths": {
                "train": None,
                "val": None,
                "test": None,
                "label_map_path": None,
            },
            "model_class": type(self.clf).__name__
        }
        with open(out_dir / "pickle_meta.json", "w", encoding="utf-8") as f:
            json.dump(meta, f, ensure_ascii=False, indent=2)
        
        logger.info(f"[SAVE] {to_relpath(out_dir)}")

    @classmethod
    def load(cls, path: Path, logger: Optional[logging.Logger]=None) -> TfidfOvRModel:
        """
        Reload model artifact from `path`. Restores vectorizers, classifier,
        and metadata (including label dimension `K`).
        """
        in_dir = Path(path)
        assert in_dir.exists(), f"[PATH] Invalid Path: {to_relpath(in_dir)}"

        if logger is None:
            logger = logging.getLogger(__name__)
        
        with open(in_dir / "pickle_meta.json", "r", encoding="utf-8") as f:
            meta = json.load(f)

        tfidf_meta = dict(meta["tfidf"])
        tfidf_meta["dtype"] = (
            np.float32 if str(tfidf_meta.get("dtype", "float32")).lower() == "float32"
            else np.dtype(tfidf_meta["dtype"]).type
        )
        tfidf_meta["word_ngram"] = tuple(tfidf_meta.get("word_ngram", [1, 2]))
        tfidf_meta["char_ngram"] = tuple(tfidf_meta.get("char_ngram", [3, 5]))
        model = cls(tfidf=TFIDFArgs(**tfidf_meta), lr=LRArgs(**meta["lr"]))

        # --- restore vecs & clf (if present) ---
        w_path = in_dir / "tfidf_word.pkl"
        c_path = in_dir / "tfidf_char.pkl"

        if w_path.exists():
            with open(w_path, "rb") as f:
                model.vw = pickle.load(f)
        
        if c_path.exists():
            with open(c_path, "rb") as f:
                model.vc = pickle.load(f)
        
        with open(in_dir / "logreg_ovr.pkl", "rb") as f:
            model.clf = pickle.load(f)
        
        k = meta.get("K", None)
        model.K = int(k) if k is not None else None

        logger.info("[LOAD] %s (protocol=%s)", to_relpath(in_dir), str(meta.get("protocol")))
        return model

# =============================================================================
# Thin factory & runner-compatible hook
# =============================================================================

def build_model(**kw) -> "ProbModel":
    """
    Factory: create TfidfOvRModel from kwargs.

    Notes
    -----
    - Values come from runner_common/CLI
    - Only parses fields relevant to TFIDFArgs/LRArgs
    """
    tfidf = TFIDFArgs(
        vec_mode=str(kw.get("vec_mode", "both")),
        min_df=kw.get("min_df", 2),
        word_ngram=tuple(kw.get("word_ngram", (1,2))),
        char_ngram=tuple(kw.get("char_ngram", (3, 5))),
        word_max=int(kw.get("word_max", 50_000)),
        char_max=int(kw.get("char_max", 30_000)),
        dtype=np.float32,
    )

    lr = LRArgs(
        C=float(kw.get("C", 2.0)),
        solver=str(kw.get("solver", "liblinear")),
        max_iter=int(kw.get("max_iter", 400)),
        n_jobs=kw.get("n_jobs", None),
        class_weight=kw.get("class_weight", None),
        random_state=kw.get("random_state", None),
    )

    model = TfidfOvRModel(tfidf=tfidf, lr=lr)
    return model

def run_from_kwargs(**kw) -> Dict[str, Any]:
    """
    Programatic runner for TF-IDF/LR.

    Parameters
    ----------
    **kw
        Mixed configuration captured from CLI/runner_common:
            - CommonArgs: run, train, val, test, label_map, protocol, seed, log_level
            - EvalArgs  : thr, threshold_policy, ks
            - TFIDF/LR  : see build_model()
    
    Returns
    -------
    Dict[str, Any]
        {
            "run"   : str,
            "model_dir": str,
            "val"   : dict | None,
            "test"  : dict | None
        }
    """
    # --- Parse & Setup ---
    common = CommonArgs(
        run=str(kw.get("run", "tfidf_lr_v1")),
        train=str(kw.get("train", "data/splits/train.jsonl")),
        val=kw.get("val", "data/splits/val.jsonl"),
        test=kw.get("test", "data/splits/test.jsonl"),
        label_map=str(kw.get("label_map", "data/processed/label_map.json")),
        protocol=int(kw.get("protocol", 4)),
        seed=int(kw.get("seed", 42)),
        log_level=str(kw.get("log_level", "INFO")),
    )

    ev = EvalArgs(
        thr=float(kw.get("thr", 0.5)),
        threshold_policy=str(kw.get("threshold_policy", "fixed")),
        ks=tuple(kw.get("ks", (8, 15))),
        min_pos=int(kw.get("min_pos", 10)),
    )

    logger = setup_logger(name=f"tfidf_lr.{common.run}")
    logger.setLevel(getattr(logging, common.log_level, logging.INFO))
    set_global_seed(common.seed)

    # --- Load splits (mini/full via runner) ---
    tr_texts, Y_tr = _load_split(common.train, label_map=common.label_map, logger=logger)
    assert isinstance(Y_tr, np.ndarray) and Y_tr.ndim == 2, "[CHECK] Y must be 2D (N, K)"

    va_texts, Y_va = [], None
    te_texts, Y_te = [], None
    if common.val:
        va_texts, Y_va = _load_split(common.val, label_map=common.label_map, logger=logger)
    if common.test:
        te_texts, Y_te = _load_split(common.test, label_map=common.label_map, logger=logger)

    # --- Build & fit model ---
    model = build_model(**kw)
    if hasattr(model, "set_pickle_protocol"):
        model.set_pickle_protocol(common.protocol)
    model.fit(tr_texts, Y_tr, logger=logger)

    model_dir = Path("models") / common.run
    model.save(model_dir, logger=logger)
    logger.info("[SAVE:done] %s", to_relpath(model_dir))

    # --- Evaluate on VAL (tune thresholds if requested) ---
    val_row: Optional[dict] = None
    if Y_va is not None and len(va_texts) > 0:
        P_va = model.predict_proba(va_texts, logger=logger)
        assert P_va.shape == Y_va.shape, "[CHECK] val proba shape mismatch"
        val_row = evaluate_and_save(
            run_name=common.run,
            split_name="val",
            y_true=Y_va,
            y_scores=P_va,
            thr=ev.thr,
            threshold_policy=ev.threshold_policy,
            metrics_csv=Path("reports/baseline_metrics.csv"),
            min_pos=ev.min_pos,
            thresholds_dir=Path("models"),
            logger=logger,
        )

    # --- Evaluate on TEST (apply thresholds) ---
    test_row: Optional[dict] = None
    if Y_te is not None and len(te_texts) > 0:
        P_te = model.predict_proba(te_texts, logger=logger)
        assert P_te.shape == Y_te.shape, "[CHECK] test proba shape mismatch"
        test_policy = "use_saved" if ev.threshold_policy in {"global_val", "per_label_val"} else ev.threshold_policy
        test_row = evaluate_and_save(
            run_name=common.run,
            split_name="test",
            y_true=Y_te,
            y_scores=P_te,
            thr=ev.thr,
            threshold_policy=test_policy,
            metrics_csv=Path("reports/baseline_metrics.csv"),
            min_pos=ev.min_pos,
            thresholds_dir=Path("models"),
            logger=logger,
        )

    result: Dict[str, Any] = {
        "run": common.run,
        "model_dir": str(to_relpath(model_dir)),
        "val": val_row,
        "test": test_row,
    }

    logger.info("[RUN:done] %s | val=%s | test=%s",
                common.run, "ok" if val_row else "skip", "ok" if test_row else "skip")
    
    return result